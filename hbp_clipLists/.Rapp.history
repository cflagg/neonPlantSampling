test[which(test$error==0.2 & test$CI.2tail=="95%"),"n"]
test[which(test$error=="0.2" & test$CI.2tail=="95%"),"n"]
sample.num2 = function(stdev, average, plot.size=NA){#
#
## Calculate the coefficient of variation from the standard deviation and mean input by the user#
CV = stdev/average#
##	Calculate plot.area in hectares from plot.size in meters#
plot.area = plot.size^2/10000#
## Create vector of t-statistics associated with 2-tailed cumulative probabilities that equal 80%, 90%, and 95%, and assuming df=infinity. Values were taken from http://en.wikipedia.org/wiki/Student's_t-distribution#
t.2tail = c("80%"=1.282, "90%"=1.645, "95%"=1.960)#
##	Define levels of error "E" for which sample-size "n" is desired#
E = c(0.1, 0.2, 0.3)#
## Define an empty matrix to hold sample number "n" values and "total area sampled" corresponding to range of errors contained in "E" above, for each value of "t.2tail" vector.#
n.mat = matrix(data=NA, nrow=length(E)*length(t.2tail), ncol=2)#
colnames(n.mat) = c("n","tot.ha")#
##	Define "error", "CI.2tail", and "t.stat" vectors, and combine into a data.frame with n.mat above.#
error = rep(E, times=length(t.2tail))#
CI.2tail = rep(names(t.2tail), each=length(E))#
t.stat = as.numeric(rep(t.2tail, each=length(E)))#
temp.df = data.frame(error, CI.2tail, t.stat, n.mat, stringsAsFactors=FALSE)#
##	Calculate sample-size "n" required for defined levels of "error" and associated values of "t.stat"; use "n" values to calculate total area sampled with user-defined "plot.size".#
temp.df$n = temp.df$t.stat^2*CV^2/temp.df$error^2#
#
#	Employ "if/else" statement to inform user if plot.size was not supplied; if plot.size is supplied, then calculate total area sampled in hectares.#
if (is.na(plot.size)){#
print("A value for plot.size was not entered, so total area sampled cannot be calculated.")#
#
} else {#
temp.df$tot.ha = temp.df$n*plot.area#
#
#	End of "if/else" statement bracket#
}#
##	Plot curves showing sample size "n" as a function of acceptable error, with a separate curve for each level of confidence specified in "t.2tail"#
#
#	Create empty plot to contain curves#
plot(temp.df$error, temp.df$n, type="n", xlab="Error (proportion of mean)", ylab="Sample size (n)", xlim=c(0.05, max(temp.df$error)), ylim=c(0, max(temp.df$n)+5), main="Relationship between error and sample size\nat various levels of confidence")#
#
#	Employ "for" loop to plot curves for each value of "t.2tail"#
for (i in 1:length(t.2tail)){#
curve(t.2tail[i]^2*CV^2/x^2, 0.05, max(E), col=i, add=TRUE)#
#	End of "for" loop bracket#
}#
#
# Add lines to the plot that illustrate the location of the "n" value associated with acceptable error = 20% at a 95% confidence level.#
v.x = c(0.2,0.2)#
v.y = c(-1, temp.df[which(temp.df$error==0.2 & temp.df$CI.2tail=="95%"),"n"])#
lines(v.x, v.y, col="gray60", lty=1)#
h.x = c(0,0.2)#
h.y = c(temp.df[which(temp.df$error==0.2 & temp.df$CI.2tail=="95%"),"n"], temp.df[which(temp.df$error==0.2 & temp.df$CI.2tail=="95%"),"n"])#
lines(h.x, h.y, col="gray60", lty=1)#
#
# Add a point to the plot that illustrates the location of the "n" value for acceptable error = 20% of the mean at a 95% confidence level.#
points(0.2, temp.df[which(temp.df$error==0.2 & temp.df$CI.2tail=="95%"),"n"], pch=21, bg="gray80")#
#
# Create legend based on confidence levels contained in "t.2tail"#
legend("topright", c("95% conf (alpha/2=0.025)", "90% conf (alpha/2=0.05)", "80% conf (alpha/2=0.10)"), col=c("green", "red", "black"), title="Confidence level (2-tail)", lty=1, cex=0.8)#
##	Clean up "temp.df" to return to user#
temp.df$n = ceiling(temp.df$n)#
temp.df$tot.ha = round(temp.df$tot.ha, digits=2)#
return(temp.df)#
#
#	End of function bracket#
}
test = sample.num2(0.257,0.825,40)
test = sample.num2(0.3,0.825,40)
test = sample.num2(0.4,0.825,40)
q()
sample.sim1 = function(plot.size, iter.num){#
#
##	Calculate subsample plot area in hectares#
plot.area = plot.size^2/10000#
##	Read in user-supplied stem map data; it is necessary to prepare the stem map with code in "Data_preparation.R" prior to sample simulation.#
print("Select file containing stem map data")#
stemmap.df = read.csv(file.choose(), header=T)#
##	Load table of Jenkins parameters to use for biomass estimation of individual stems. Parameters come from Jenkins etal. 2003 Forest Science.#
print("Select file containing allometric parameters from Jenkins et al. 2003 Forest Science")#
jpar.df = read.csv(file.choose(), header=T)#
#
#	Create look-up array that matches "Jenkins_type" to the correct row in "jpar.df"#
jpar.LUT = 1:nrow(jpar.df)#
names(jpar.LUT) = jpar.df$groupID#
##	Plot location of stems in "stemmap.df"#
plot(stemmap.df$xdist, stemmap.df$ydist, type="n", xlab="Relative Easting (m)", ylab="Relative Northing (m)", main="Stem map with symbol size = relative stem DBH", cex.main=0.9)#
#
#	Add points with symbols sized according to DBH#
symbols(stemmap.df$xdist, stemmap.df$ydist, circles=stemmap.df$dbh.cm, inches=0.1, add=TRUE)#
##	Define spatial limits for random sub-sampling routine, leaving a buffer so that subsampling plots do not overlap the edge of the stemmap.df dataset.#
max.x = max(stemmap.df$xdist) - plot.size/2#
min.x = min(stemmap.df$xdist) + plot.size/2#
max.y = max(stemmap.df$ydist) - plot.size/2#
min.y = min(stemmap.df$ydist) + plot.size/2#
##	Create "agb.sampled" vector to hold AGB for each subsampling iteration#
agb.sampled = NA#
##	Sample-iteration "for" loop used to obtain AGB distribution according to user-specified plot.size; loop iterates n=iter.num times.#
#
for (i in 1:iter.num){#
#
##	Generate random X and Y sampling coordinates, leaving a buffer so that subsampling plots do not overlap the edge of the stemmap.df dataset.#
random.x = round(runif(1, min.x, max.x), digits=1)#
random.y = round(runif(1, min.y, max.y), digits=1)#
##	Plot random.x and random.y coordinates sampled for each iteration of loop#
symbols(random.x, random.y, squares=plot.size, inches=FALSE, add=TRUE, fg=4)#
##	Define "xdist" and "ydist" range within "stemmap.df" associated with "random.x" and "random.y" coordinates based on plot.size; filter first by "xdist" then by "ydist" to obtain a temporary matrix containing only those stems that fall within the randomly selected coordinate range.#
temp.stem = stemmap.df[which(stemmap.df$xdist >= random.x-plot.size/2 & stemmap.df$xdist < random.x+plot.size/2),]#
temp.stem = temp.stem[which(temp.stem$ydist >= random.y-plot.size/2 & temp.stem$ydist < random.y+plot.size/2),]#
##	Calculate the biomass of each stem in "temp.stem", sum biomass across all randomly subsampled stems, and store the result in "agb.sampled". Use "if/else" statement to deal with the case of no stems occurring within the random subsampling location.#
#
if (nrow(temp.stem) == 0){#
#
#	Assign AGB=0 if there are no stems within the random subsampling plot#
agb.sampled[i] = 0#
#
} else {#
#
#	Create temporary vector to store AGB values for each stem#
agb.temp = NA#
#
#	When stems exist in "temp.stem", use nested stem-mass "for" loop to calculate AGB for each stem "j" in "temp.stem"#
#
for (j in 1:nrow(temp.stem)){#
#
#	Retrieve correct Jenkins parameters for the stem based on "Jenkins_type" code#
temp.b0 = jpar.df[jpar.LUT[[as.character(temp.stem$Jenkins_type[j])]], "b0"]#
temp.b1 = jpar.df[jpar.LUT[[as.character(temp.stem$Jenkins_type[j])]], "b1"]#
#
#	Using Jenkins AGB equation, calculate the AGB in kg for stem "j" using DBH value for stem "j" in "temp.stem"#
agb.stem = exp(temp.b0 + temp.b1*log(temp.stem$dbh.cm[j]))#
#
#	Store AGB in kg for each stem "j"#
agb.temp[j] = agb.stem#
#
#	Bracket for end of stem-mass "for" loop#
}#
#
##	Calculate sum of biomass for all stems in randomly subsampled plot (kg), and convert to Mg ha-1 using plot.area; then store this value in "agb.sampled"#
agb.sampled[i] = sum(agb.temp)/1000/plot.area#
#	Bracket for end of sample-iteration "else" statement#
}#
#
#	Bracket for end of sample-iteration "for" loop#
}#
##	Calculate parameters required by "Sample_number_function2.R", and determine whether "agb.sampled" distribution is normal using both the Anderson-Darling normality test, and the Shapiro-Wilk normality test.#
agb.mean = mean(agb.sampled)#
agb.sd = sd(agb.sampled)#
require(nortest)#
AD = ad.test(agb.sampled)#
#
if (iter.num > 5000){#
SW = NA#
} else {#
SW = shapiro.test(agb.sampled)#
}#
#
##	Plot sampled distribution of AGB in a new window, and overlay a normal distribution with same mean and standard deviation as "agb.sampled"#
quartz()#
hist(agb.sampled, main="Distribution of sampled AGB (histogram)\nand normal distribution (red)", xlab="AGB (Mg ha-1)", prob=TRUE)#
curve(dnorm(x, mean=agb.mean, sd=agb.sd), add=TRUE, col=2)#
##	Bundle agb.sampled, mean, sd, and Anderson-Darling test results into a list, and return the list to the user#
results = list(agb=agb.sampled, mean=agb.mean, sd=agb.sd, ad=AD, sw=SW)#
return(results)#
#	Bracket for end of function#
}
test = sample.sim1(20,500)
names(test)
test$mean
test$sd
test$ad
test$sw
q()
weight = c(0.2,-0.5,-1.3,-1.6,-0.7,0.4,-0.1,0,-0.6,-1.1,-1.2,-0.8)
var(weight)
sd(weight)/mean(weight)
sd(weight)/abs(mean(weight))
faithful
?faithful
rm(list=ls())
q()
?reshape
q()
library(geoR)
q()
install.views("Spatial")
?install.views
??install.views
library(ctv)
install.views("Spatial")
q()
ls()
i
q()
?symbols
q()
install.packages(c("latticeExtra","ramps","lattice","sp","rgdal","gridExtra",#
"MCMCpack","mvtnorm","SuppDists","gstat"),dependencies=T)
)
require(rgdal)#
require(gstat)#
require(latticeExtra)#
require(gridExtra)#
require(ramps)#
require(lattice)#
require(sp)#
require(MCMCpack)#
require(mvtnorm)#
require(SuppDists)
q()
ls()
require(rgdal)#
require(gstat)#
require(latticeExtra)#
require(gridExtra)#
require(ramps)#
require(lattice)#
require(sp)#
require(MCMCpack)#
require(mvtnorm)#
require(SuppDists)
install.packages(c("latticeExtra","ramps","lattice","sp","rgdal","gridExtra",#
"MCMCpack","mvtnorm","SuppDists","gstat"),dependencies=T)
require(rgdal)#
require(gstat)#
require(latticeExtra)#
require(gridExtra)#
require(ramps)#
require(lattice)#
require(sp)#
require(MCMCpack)#
require(mvtnorm)#
require(SuppDists)
load("/Users/cmeier/Documents/R_resources/Duffy_simulation_design_201305/base_051313.Rdata")
ls()
rm(list=ls())
ls()
load("/Users/cmeier/Documents/R_resources/Duffy_simulation_design_201305/base_051313.Rdata")
ls()
quartz()
i=1
i
tmp.name<-paste("historical_harv_data","_",i,sep="")
tmp.name
library(gstat)
spattemp.mcmc(yr.begin=3,yr.end=5,data=harv.hist.dat,formula="beta[1]+beta[2]*t",separable=T, #
name=tmp.name,burnin=10000,mcmc=10000,thin=10,update.interval=1,tune=c(1,1,1,1,1,1),plotit=T)
head(harv.hist.dat)
q()
library(nlme)
?VarIdent
?varident
??varident
?varIdent
q()
sample.num2 = function(stdev, average, plot.size=NA){#
#
## Calculate the coefficient of variation from the standard deviation and mean input by the user#
CV = stdev/average#
##	Calculate plot.area in hectares from plot.size in meters#
plot.area = plot.size^2/10000#
## Create vector of t-statistics associated with 2-tailed cumulative probabilities that equal 80%, 90%, and 95%, and assuming df=infinity. Values were taken from http://en.wikipedia.org/wiki/Student's_t-distribution#
t.2tail = c("80%"=1.282, "90%"=1.645, "95%"=1.960)#
##	Define levels of error "E" for which sample-size "n" is desired#
E = c(0.1, 0.2, 0.3)#
## Define an empty matrix to hold sample number "n" values and "total area sampled" corresponding to range of errors contained in "E" above, for each value of "t.2tail" vector.#
n.mat = matrix(data=NA, nrow=length(E)*length(t.2tail), ncol=2)#
colnames(n.mat) = c("n","tot.ha")#
##	Define "error", "CI.2tail", and "t.stat" vectors, and combine into a data.frame with n.mat above.#
error = rep(E, times=length(t.2tail))#
CI.2tail = rep(names(t.2tail), each=length(E))#
t.stat = as.numeric(rep(t.2tail, each=length(E)))#
temp.df = data.frame(error, CI.2tail, t.stat, n.mat, stringsAsFactors=FALSE)#
##	Calculate sample-size "n" required for defined levels of "error" and associated values of "t.stat"; use "n" values to calculate total area sampled with user-defined "plot.size".#
temp.df$n = temp.df$t.stat^2*CV^2/temp.df$error^2#
#
#	Employ "if/else" statement to inform user if plot.size was not supplied; if plot.size is supplied, then calculate total area sampled in hectares.#
if (is.na(plot.size)){#
print("A value for plot.size was not entered, so total area sampled cannot be calculated.")#
#
} else {#
temp.df$tot.ha = temp.df$n*plot.area#
#
#	End of "if/else" statement bracket#
}#
##	Plot curves showing sample size "n" as a function of acceptable error, with a separate curve for each level of confidence specified in "t.2tail"#
#
#	Create empty plot to contain curves#
plot(temp.df$error, temp.df$n, type="n", xlab="Error (proportion of mean)", ylab="Sample size (n)", xlim=c(0.05, max(temp.df$error)), ylim=c(0, max(temp.df$n)+5), main="Relationship between error and sample size\nat various levels of confidence")#
#
#	Employ "for" loop to plot curves for each value of "t.2tail"#
for (i in 1:length(t.2tail)){#
curve(t.2tail[i]^2*CV^2/x^2, 0.05, max(E), col=i, add=TRUE)#
#	End of "for" loop bracket#
}#
#
# Add lines to the plot that illustrate the location of the "n" value associated with acceptable error = 20% at a 95% confidence level.#
v.x = c(0.2,0.2)#
v.y = c(-1, temp.df[which(temp.df$error==0.2 & temp.df$CI.2tail=="95%"),"n"])#
lines(v.x, v.y, col="gray60", lty=1)#
h.x = c(0,0.2)#
h.y = c(temp.df[which(temp.df$error==0.2 & temp.df$CI.2tail=="95%"),"n"], temp.df[which(temp.df$error==0.2 & temp.df$CI.2tail=="95%"),"n"])#
lines(h.x, h.y, col="gray60", lty=1)#
#
# Add a point to the plot that illustrates the location of the "n" value for acceptable error = 20% of the mean at a 95% confidence level.#
points(0.2, temp.df[which(temp.df$error==0.2 & temp.df$CI.2tail=="95%"),"n"], pch=21, bg="gray80")#
#
# Create legend based on confidence levels contained in "t.2tail"#
legend("topright", c("95% conf (alpha/2=0.025)", "90% conf (alpha/2=0.05)", "80% conf (alpha/2=0.10)"), col=c("green", "red", "black"), title="Confidence level (2-tail)", lty=1, cex=0.8)#
##	Clean up "temp.df" to return to user#
temp.df$n = ceiling(temp.df$n)#
temp.df$tot.ha = round(temp.df$tot.ha, digits=2)#
return(temp.df)#
#
#	End of function bracket#
}
q()
x = rnorm(4)
x
x = rnorm(100)
x
sd(x)
x = rnorm(1000)
sd(x)
mean(x)
(1.96/0.2)^2
q()
getcwd()
cwd()
??working directory
getwd()
q()
library(TeachingDemos)
?TeachingDemos
?char2seed
char2seed(HARV_001, set=FALSE)
char2seed("HARV_001", set=FALSE)
temp = c("HARV_001","HARV_002","HARV_003")
char2seed(temp, set=FALSE)
char2seed(temp[1], set=FALSE)
char2seed(temp[2], set=FALSE)
char2seed(temp[3], set=FALSE)
char2seed("HARV", set=FALSE)
temp2 = rnorm(100)
temp
temp2
?sample
sample(temp2, size=5)
char2seed("HARV", set=TRUE)
sample(temp2, size=5)
char2seed("HARV", set=TRUE)
sample(temp2, size=5)
char2seed("HARV", set=TRUE); sample(temp2, size=5)
q()
ls()
rm(list=ls())
ls()
q()
ls()
q()
q25 = c(4.39,3.08,4.43)
q10 = c(3.88,4.07,3.93)
?t.test
t.test(q25,q10)
mean(q25)
mean(q10)
ls()
q()
test = c("test1","test2","test3")
test4 %in% test
"test4" %in% test
"test3" %in% test
basename()
q()
binomialsampsize = function( p0, b1, maxtime, sampfreq, timerange=0,#
                             sigLevel=0.05, power=0.9, pooled = FALSE,#
                             poolsize = NULL){#
  ## p0 is vector of baseline rates or probabilities#
  ## b1 is vector of trend values#
  ## maxtime is a vector of different maximum lenghts of time (years)#
  ## sampfreq is the number of sampling intervals per unit of time#
  ## timerange is an exponential-decay correlation parameter#
  ## pooled indicates whether the samples are pooled#
  ## poolsize is the number of samples in each pool#
  # p0 = 0.1; b1 = 0.05; maxtime = 10; sampfreq = 0.5; timerange=0; sigLevel=0.05; power=0.9; pooled = TRUE; poolsize = 50#
  ## Convert p0 to b0#
  if(!pooled){#
    b0 = log(p0/(1-p0))#
  }#
  if(pooled){ # See Farrington article for derivation.#
    b0 = log( (-1) * log(1 - p0) )#
    # An approximation of this for small p0 is b0 = log(p0)#
  }#
  ## Precalculate re-used quantity#
  zpre = qnorm(1-sigLevel) + qnorm(power)#
  ## Set up storage#
  narr = array( dim=c(length(sampfreq),length(maxtime),length(p0),length(b1)) )#
  ## Loop through different specified sampling frequenciess#
  for(i in 1:length(sampfreq)){#
    ## Loop through different specified time limits#
    for(j in 1:length(maxtime)){#
      ## Set up sequence of sampling times#
      tm = seq( 0, maxtime[j], sampfreq[i] )#
      ## Compute temporal correlation matrix#
      if( timerange==0 ){#
        cormat = diag( 1, length(tm) )#
      } else {#
        tmp = as.matrix( dist(tm,upper=TRUE,diag=TRUE) )#
        cormat = exp( -tmp / timerange )#
      }#
      ## Construct design matrix for regression#
      if(!pooled) xmat = cbind(1,tm)#
      if(pooled) xmat = cbind(log(poolsize),1,tm)#
      ## Loop through different specified intercept terms#
      for(k in 1:length(p0)){#
        ## Loop through different specified slop terms#
        for(m in 1:length(b1)){#
          ## Compute regression curve#
          if(!pooled) regrFits = as.vector( xmat %*% c( b0[k], b1[m] ) )#
          if(pooled) regrFits = as.vector( xmat %*% c(1, b0[k], b1[m] ) )#
          ## Convert to sampling mean#
          if(!pooled){#
            tmp = exp(regrFits)#
            p = tmp / (1+tmp) #
          }#
          if(pooled){#
            p = 1 - exp( (-1) * exp( regrFits ) )#
          }#
          ## Construct variance and standard deviation matrix#
          wmat = diag( p * (1-p) )#
          wrootmat = diag( sqrt( p * (1-p) ) )#
          ## Compute standard error of estimate based on#
          ##   sample size of 1#
          if(!pooled){#
            xtxinv = solve( t( xmat ) %*% wmat %*% xmat )#
            ses = xtxinv %*% t(xmat) %*% wrootmat %*% cormat %*% wrootmat %*% xmat %*% xtxinv#
          }#
          if(pooled){ # The first column is the offset and doesn't have variance.#
            xtxinv = solve( t( xmat[,-1] ) %*% wmat %*% xmat[,-1] )#
            ses = xtxinv %*% t(xmat[,-1]) %*% wrootmat %*% cormat %*% wrootmat %*% xmat[,-1] %*% xtxinv#
          }#
          ## Compute required sample size#
          narr[i,j,k,m] = ceiling( ( zpre / b1[m] )^2 * ses[2,2] )#
        }#
      }#
    }#
  }#
  ## Label output and return#
  dimnames(narr) = list(sampfreq,maxtime,p0,b1)#
  return(narr)#
}
binomialsampsize(0.1,0.05,10, 0.5, timerange=0, sigLevel=0.05, power=0.9, pooled=TRUE, poolsize=50)
test = binomialsampsize(0.1,0.05,10, 0.5, timerange=0, sigLevel=0.05, power=0.9, pooled=TRUE, poolsize=50)
test
str(test)
binomialsampsize(0.1,0.05,10,0.5,0,0.05,0.9,TRUE,50)
q()
ls()
rm(list=ls())
q()
?for
for (i in 1:10){#
print("I rule")#
}
test = NA#
for (i in 1:10){#
append(test)[i]#
}
test = NA#
for (i in 1:10){#
test[i] = i#
}
test
test = NA#
for (i in 1:10){#
test[i] = i/pi#
}
test
for (i in 1:10){#
print(paste("I love the number",[i]))#
}
?paste
paste("Today is",1)
for (i in 1:10){#
print(paste("I love the number",i))#
}
ls()
rm(list=ls())
q()
ls()
q()
ls()
q()
df = data.frame(matrix(rnorm(20), nrow=10))
df
df[sample(nrow(df),4),]
df[sort(sample(nrow(df),4)),]
df = data.frame(matrix(runif(30), ncol=3, nrow=10))
df
df[sort(sample(nrow(df),4)),]
?sample
q()
df = data.frame(matrix(runif(30), ncol=3, nrow=10))
df
plot(density(rnorm(1000)))
plot(density(runif(1000)))
hist(runif(1000))
hist(rnorm(1000))
?sample
sample(rnorm(1000), 10, replace=FALSE)
df[sample(nrow(df),4, replace=FALSE),]
df[sort(sample(nrow(df),4, replace=FALSE)),]
set.seed = 42
df[sort(sample(nrow(df),4, replace=FALSE)),]
set.seed = 25
df[sort(sample(nrow(df),4, replace=FALSE)),]
set.seed = 42
df[sort(sample(nrow(df),4, replace=FALSE)),]
?set.seed
x = 1:5
y = 6:10
x
y
z = sample(x,1) + sample(y,1)
z
tempX = sample(x,1)
tempY = sample(y,1)
tempX
tempY
z = tempX + tempY
z
for (i in 1:nrow(df)){#
x1 = sample(x,1)#
y1 = sample(y,1)#
z = x1 + y1#
return(z)#
}
for (i in 1:nrow(df)){#
x1 = sample(x,1)#
y1 = sample(y,1)#
z = x1 + y1#
}
for (i in 1:nrow(df)){#
x1 = sample(x,1)#
y1 = sample(y,1)#
z = x1 + y1#
print(z)#
}
nrow(df)
xy.mat = matrix(data=NA, ncol=3, nrow=nrow(df))
xy.mat
for (i in 1:nrow(df)){#
x1 = sample(x,1)#
y1 = sample(y,1)#
z = x1 + y1#
xy.mat[i,1] = x1#
xy.mat[i,2] = y1#
xy.mat[i,3] = z#
}
xy.mat
tempList = paste("HARV",1:10, sep="_")
tempList
tempList[1]
q()
var(rnorm(10))
?var
q()
testRho = density(rnorm(1000))
hist(rnorm(1000), prob=TRUE)
hist(rnorm(1000), prob=FALSE)
lines(testRho, col=3)
lines(testRho, col=4)
lines(testRho, col=2)
q()
test = matrix(data=NA, nrow=2, ncol=2)
test = data.frame(test)
colnames(test) = c("Col1","Col2")
test
colnames(test)["Col2"] = "ColBlah"
colnames(test)[2] = "ColBlah"
test
colnames(test)[2] = "Col2"
test
colnames(test)[test$Col2] = "Blarg"
test
colnames(test)[""Col2""] = "ColBlah"
colnames(test)[[Col2]] = "ColBlah"
test
colnames(test$Col2) = "Blarg"
colnames(test["Col2"]) = "Blarg"
test
names(test$Col2) = "Blarg"
test
names(test[,"Col2"]) = "Blarg"
test
colnames(test)[colnames(test)=="Col2"] = "Blarg"
test
q()
library(gplots)
textplot( matrix(letters[1:26], ncol=2))
?pdf
q()
ls()
rm(list=ls())
q()
source("Clip_list_function.R")
clipList(20, "Distributed","JERC")
clipList(40, "Tower","JERC")
q()
q()
